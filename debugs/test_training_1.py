"""
Test complete training pipeline phase 1 on small dataset
These testcase runs a mini training loop on a tiny dataset (10 images)
to verify that all components (data loading, model forward, loss computation,
backward, optimizer step) work together without errors.
This is not a full training test, just a sanity check.

==== Generated by Claude with prompt, finalized by author (This specific testcase ONLY) ====
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import torch
from torch.utils.data import DataLoader, Subset
from src.dataset import CrossViewDataset
from src.models.crossview_model import CrossViewModel
from src.models.teachers import FrozenTeachers
from src.losses import CombinedLoss
import random

def test_training_pipeline():
    """Test complete training pipeline"""
    
    print("=" * 60)
    print("MINI TRAINING TEST (10 images, 5 iterations)")
    print("=" * 60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device}\n")
    
    # 1. Load tiny dataset
    print("Step 1: Loading dataset...")
    full_dataset = CrossViewDataset(
        root="data/University-Release",
        split='train',
        mode='drone',
        use_augmentation=True,
        use_first_image=True
    )
    
    # Use only 10 images for test
    tiny_dataset = Subset(full_dataset, range(10))
    loader = DataLoader(tiny_dataset, batch_size=4, shuffle=True, num_workers=0)
    
    print(f"✓ Loaded {len(tiny_dataset)} samples")
    
    # 2. Create model
    print("\nStep 2: Creating model...")
    model = CrossViewModel(pretrained=False).to(device)
    print(f"✓ Model created ({sum(p.numel() for p in model.parameters())/1e6:.1f}M params)")
    
    # 3. Load teachers
    print("\nStep 3: Loading teachers...")
    teachers = FrozenTeachers(device=device)
    print(f"✓ Teachers loaded")
    
    # 4. Create loss and optimizer
    print("\nStep 4: Setting up training...")
    criterion = CombinedLoss(lambda_clip=0.5, lambda_dino=0.3, alpha=10.0)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.03)
    
    print(f"✓ Optimizer: AdamW (lr=1e-4)")
    
    # 5. Mini training loop
    print("\nStep 5: Running mini training (5 iterations)...")
    print("-" * 60)
    
    model.train()
    
    for iteration in range(5):
        # Get batch
        street_img, drone_img, building_ids, univ_ids, build_nums = next(iter(loader))
        street_img = street_img.to(device)
        drone_img = drone_img.to(device)
        
        # Sample negative drones (random shuffle)
        indices = torch.randperm(len(drone_img))
        drone_neg = drone_img[indices]
        
        # Forward pass: student model
        street_clip, street_dino, drone_clip, drone_dino = model(
            street_img, drone_img, return_both_embeddings=True
        )
        
        # Get negative embeddings
        _, _, drone_neg_clip, _ = model(
            street_img, drone_neg, return_both_embeddings=True
        )
        
        # Forward pass: teachers (frozen)
        teacher_features = teachers(street_img, drone_img)
        
        # Prepare outputs for loss
        student_outputs = {
            'street_clip': street_clip,
            'street_dino': street_dino,
            'drone_clip': drone_clip,
            'drone_dino': drone_dino,
            'drone_neg_clip': drone_neg_clip
        }
        
        # Compute losses
        losses = criterion(student_outputs, teacher_features)
        
        # Backward pass
        optimizer.zero_grad()
        losses['total'].backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # Optimizer step
        optimizer.step()
        
        # Log
        print(f"Iter {iteration+1}/5: "
              f"Total={losses['total'].item():.4f} | "
              f"Triplet={losses['triplet'].item():.4f} | "
              f"CLIP={losses['clip'].item():.4f} | "
              f"DINO={losses['dino'].item():.4f}")
    
    print("-" * 60)
    print("\n✓ Mini training completed successfully!")
    print("\nWhat this proves:")
    print("  ✓ Model forward pass works")
    print("  ✓ Teacher encoding works")
    print("  ✓ Loss computation works")
    print("  ✓ Backward pass works")
    print("  ✓ Optimizer step works")
    print("  ✓ Gradient clipping works")
    print("\n✓ Ready for full training!")
    
    print("=" * 60)
    print("TESTING CHECKPOINT SAVING")
    print("=" * 60)

    # Save checkpoint
    checkpoint_path = Path("checkpoints/test_checkpoint.pth")
    checkpoint_path.parent.mkdir(exist_ok=True)

    checkpoint = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': 5,
        'best_recall': 0.85,  # Dummy value
        'config': {
            'lambda_clip': 0.5,
            'lambda_dino': 0.3,
            'num_universities': 36,
            'num_buildings': 701
        }
    }

    torch.save(checkpoint, checkpoint_path)
    print(f"✓ Saved checkpoint to: {checkpoint_path}")

    # Test loading
    print("\nTesting checkpoint loading...")
    loaded = torch.load(checkpoint_path)
    print(f"✓ Checkpoint loaded")
    print(f"  Keys: {list(loaded.keys())}")

    # Test loading into new model
    new_model = CrossViewModel(pretrained=False)
    new_model.load_state_dict(loaded['model_state_dict'])
    new_model = new_model.to(device)
    print(f"✓ Model state loaded successfully")

    # Verify it works
    new_model.eval()
    with torch.no_grad():
        test_out = new_model(street_img, drone_img, return_both_embeddings=False)
        print(f"✓ Loaded model inference works: {test_out[0].shape}")

    print("\n✓ Checkpoint save/load test passed!")
    print("✓ Phase 1 to Phase 2 compatibility verified!")


if __name__ == "__main__":
    test_training_pipeline()